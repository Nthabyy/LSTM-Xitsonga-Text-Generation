{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hlaya xiletelo xa xikombelo eka pheji 7 hi vukheta, tani hilaha xi nga na mahungu ya nkoka lama nga ta ku pfuna eka ku tata fomo ya xikombelo kahle.\\n', 'tata fomo ya xikombelo leyi nga laha ndzhaku no vona leswaku u nghenisile tidokhumente hinkwato leti lavekaka na nsayino wa wena laha wu lavekaka.\\n', \"Rhumela fomo na tidokhumente tin'wana to engetela eka gEMs hi yin'wana ya tindlela leti landzelaka:\\n\", 'Fekisi: 0861 00 4367\\n', \"Tisenthara ta xifundza to yisa hi voko: Languta xiletelo xa xikombelo eka pheji 7 ku kuma vuxokoxoko byin'wana.\\n\", 'Loko xikombelo xa wena xi amukeriwile, GEMS yi ta ku rhumela phasela ro ku amukela eka masiku ya 7 endzhaku ka ku amukela xikombelo xa wena.\\n', 'Xikombelo xa wena xi ta hlwela ku kambisisiwa loko u nga nyiki GEMS tidokhumente hinkwato leti lavekaka.\\n', 'Loko xikombelo xa wena xi nga kambisisiwangi, GEMS yi ta tihlanganisa na wena eka masiku ya 15\\n', 'ya ku amukela xikombelo xa wena.\\n', 'Tihlanganise na senthara ya hina ya tiqingho eka 0860 00 4367 kumbe u rhumela imeyili eka enquiries@gems.gov.za loko u lava ku pfuniwa nakambe mayelana na matatelo ya fomo ya wena ya xikombelo.\\n']\n"
     ]
    }
   ],
   "source": [
    "folder_path = r\"C:\\Backup\\Desktop\\Xitsonga-Text Generation-LSTM\\dataset\"\n",
    "\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "data = []\n",
    "for file_name in files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    if os.path.isfile(file_path):  \n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data.extend(file.readlines())\n",
    "\n",
    "print(data[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Existing stopwords\n",
    "xitsonga_stopwords = [\n",
    "    \"ku\", \"a\", \"i\", \"e\", \"o\", \"le\", \"ti\", \"to\", \n",
    "    \"na\", \"ni\", \"ka\", \"va\", \"hi\", \"lo\", \"ya\", \"ma\"\n",
    "]\n",
    "\n",
    "# Generate all consonant-vowel and vowel-consonant combinations\n",
    "vowels = ['a', 'e', 'i', 'o', 'u']\n",
    "consonants = [chr(c) for c in range(ord('a'), ord('z') + 1) if chr(c) not in vowels]\n",
    "\n",
    "# Add consonant-vowel and vowel-consonant combinations\n",
    "combinations = set(\n",
    "    [\"\".join(pair) for pair in itertools.product(consonants, vowels)] +\n",
    "    [\"\".join(pair) for pair in itertools.product(vowels, consonants)]\n",
    ")\n",
    "\n",
    "# Combine with existing stopwords\n",
    "xitsonga_stopwords.extend(combinations)\n",
    "\n",
    "# Ensure no duplicates\n",
    "xitsonga_stopwords = list(set(xitsonga_stopwords))\n",
    "\n",
    "# Filter stopwords to only include words with less than 3 characters\n",
    "xitsonga_stopwords = [word for word in xitsonga_stopwords if len(word) < 3]\n",
    "\n",
    "# Sort for easier debugging\n",
    "xitsonga_stopwords.sort()\n",
    "\n",
    "print(xitsonga_stopwords[:50])  # Display a subset to check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Line: Hlaya xiletelo xa xikombelo eka pheji 7 hi vukheta, tani hilaha xi nga na mahungu ya nkoka lama nga ta ku pfuna eka ku tata fomo ya xikombelo kahle.\n",
      "\n",
      "Cleaned Line: hlaya xiletelo xikombelo eka pheji 7 vukheta, tani hilaha nga mahungu nkoka lama nga pfuna eka tata fomo xikombelo kahle.\n"
     ]
    }
   ],
   "source": [
    "# Define the function to remove Xitsonga stopwords\n",
    "def remove_stopwords(text, stopwords):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Split text into words\n",
    "    words = text.split()\n",
    "    # Remove words that are in the stopwords list\n",
    "    filtered_words = [word for word in words if word not in stopwords]\n",
    "    # Join back into a string\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "# Apply the stopword removal to the dataset\n",
    "cleaned_data_without_stopwords = [remove_stopwords(line, xitsonga_stopwords) for line in data]\n",
    "\n",
    "# Preview the cleaned dataset\n",
    "print(\"Original Line:\", data[0])  # Original line for reference\n",
    "print(\"Cleaned Line:\", cleaned_data_without_stopwords[0])  # Line after stopword removal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hlaya xiletelo xikombelo eka pheji 7 vukheta, tani hilaha nga mahungu nkoka lama nga pfuna eka tata fomo xikombelo kahle.\n",
      "hlaya xiletelo xikombelo eka pheji vukheta tani hilaha nga mahungu nkoka lama nga pfuna eka tata fomo xikombelo kahle\n"
     ]
    }
   ],
   "source": [
    "# defining a function that will remove the wtitespace, convert into  lowercase\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  \n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the stopword-filtered data\n",
    "fully_cleaned_data = [clean_text(line) for line in cleaned_data_without_stopwords]\n",
    "\n",
    "# Preview the results\n",
    "print( cleaned_data_without_stopwords[0])\n",
    "print( fully_cleaned_data[0])  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Fit the tokenizer on the cleaned data to build a word index\n",
    "tokenizer.fit_on_texts(fully_cleaned_data)\n",
    "\n",
    "# Convert the cleaned text into sequences of tokens (integers)\n",
    "sequences = tokenizer.texts_to_sequences(fully_cleaned_data)\n",
    "\n",
    "# Determine the maximum sequence length (e.g. choose 30 for this case)\n",
    "max_sequence_length = 30  \n",
    "\n",
    "# Padding  the sequences to ensure they all have the same length\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "\n",
    "print(\"Padded Sequences:\")\n",
    "print(padded_sequences[:10])  # Display first 10 sequences of the paaded sequence\n",
    "\n",
    "# Check the tokenizer word index and size of the vocabulary\n",
    "print(\"\\nTokenizer Word Index:\")\n",
    "print(tokenizer.word_index)\n",
    "print(\"\\nVocabulary Size:\", len(tokenizer.word_index) + 1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: X_train shape = (1452, 29), y_train shape = (1452,)\n",
      "Validation Data: X_val shape = (182, 29), y_val shape = (182,)\n",
      "Test Data: X_test shape = (182, 29), y_test shape = (182,)\n"
     ]
    }
   ],
   "source": [
    "X = padded_sequences[:, :-1]\n",
    "y = padded_sequences[:, -1]   # The last word (target)\n",
    "\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Print the shapes of the datasets\n",
    "print(f\"Training Data: X_train shape = {X_train.shape}, y_train shape = {y_train.shape}\")\n",
    "print(f\"Validation Data: X_val shape = {X_val.shape}, y_val shape = {y_val.shape}\")\n",
    "print(f\"Test Data: X_test shape = {X_test.shape}, y_test shape = {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model architecture\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding layer\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, \n",
    "                    output_dim=150, \n",
    "                    input_length=X_train.shape[1]))\n",
    "\n",
    "# LSTM layer\n",
    "model.add(LSTM(units=256, return_sequences=False, dropout=0.3, recurrent_dropout=0.1))\n",
    "\n",
    "# Dense layer for output (softmax activation for multi-class classification)\n",
    "model.add(Dense(len(tokenizer.word_index) + 1, activation='softmax'))\n",
    "\n",
    "# Compile the model optimized adam, learning rate, and metric parameters\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 176ms/step - accuracy: 0.3124 - loss: 8.2486 - val_accuracy: 0.7198 - val_loss: 8.1846\n",
      "Epoch 2/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 130ms/step - accuracy: 0.7995 - loss: 8.1330 - val_accuracy: 0.7253 - val_loss: 7.9215\n",
      "Epoch 3/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 133ms/step - accuracy: 0.7902 - loss: 7.6894 - val_accuracy: 0.7253 - val_loss: 6.7732\n",
      "Epoch 4/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 134ms/step - accuracy: 0.7788 - loss: 6.2974 - val_accuracy: 0.7253 - val_loss: 5.2663\n",
      "Epoch 5/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 136ms/step - accuracy: 0.8034 - loss: 4.6488 - val_accuracy: 0.7253 - val_loss: 4.1728\n",
      "Epoch 6/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 135ms/step - accuracy: 0.7797 - loss: 3.5966 - val_accuracy: 0.7253 - val_loss: 3.3164\n",
      "Epoch 7/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 131ms/step - accuracy: 0.7944 - loss: 2.6463 - val_accuracy: 0.7253 - val_loss: 2.7834\n",
      "Epoch 8/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 129ms/step - accuracy: 0.8124 - loss: 1.9827 - val_accuracy: 0.7253 - val_loss: 2.5489\n",
      "Epoch 9/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 126ms/step - accuracy: 0.7984 - loss: 1.8131 - val_accuracy: 0.7253 - val_loss: 2.4687\n",
      "Epoch 10/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 132ms/step - accuracy: 0.7892 - loss: 1.7706 - val_accuracy: 0.7253 - val_loss: 2.4444\n",
      "Epoch 11/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 130ms/step - accuracy: 0.8161 - loss: 1.5294 - val_accuracy: 0.7253 - val_loss: 2.4383\n",
      "Epoch 12/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 133ms/step - accuracy: 0.7985 - loss: 1.6185 - val_accuracy: 0.7253 - val_loss: 2.4370\n",
      "Epoch 13/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step - accuracy: 0.8026 - loss: 1.5780 - val_accuracy: 0.7253 - val_loss: 2.4372\n",
      "Epoch 14/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 121ms/step - accuracy: 0.7963 - loss: 1.6070 - val_accuracy: 0.7253 - val_loss: 2.4381\n",
      "Epoch 15/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step - accuracy: 0.7885 - loss: 1.6593 - val_accuracy: 0.7253 - val_loss: 2.4397\n",
      "Epoch 16/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step - accuracy: 0.7757 - loss: 1.7278 - val_accuracy: 0.7253 - val_loss: 2.4422\n",
      "Epoch 17/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 122ms/step - accuracy: 0.7840 - loss: 1.6485 - val_accuracy: 0.7253 - val_loss: 2.4432\n",
      "Epoch 18/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 122ms/step - accuracy: 0.7951 - loss: 1.5753 - val_accuracy: 0.7253 - val_loss: 2.4447\n",
      "Epoch 19/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - accuracy: 0.7971 - loss: 1.5531 - val_accuracy: 0.7253 - val_loss: 2.4470\n",
      "Epoch 20/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 134ms/step - accuracy: 0.7996 - loss: 1.5342 - val_accuracy: 0.7253 - val_loss: 2.4492\n",
      "Epoch 21/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step - accuracy: 0.8024 - loss: 1.5105 - val_accuracy: 0.7253 - val_loss: 2.4516\n",
      "Epoch 22/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 124ms/step - accuracy: 0.7984 - loss: 1.5267 - val_accuracy: 0.7253 - val_loss: 2.4535\n",
      "Epoch 22: early stopping\n",
      "Restoring model weights from the end of the best epoch: 12.\n"
     ]
    }
   ],
   "source": [
    "# Defining EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', # monitoring the valdidation loss\n",
    "                               patience=10,  # Number of epochs with no improvement before stopping\n",
    "                               verbose=1,\n",
    "                               restore_best_weights=True) # when training staert to overfit\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=100,  \n",
    "                    batch_size=128,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def generate_text(model, tokenizer, seed_text, max_sequence_len, num_words_to_generate=50, temperature=1.0):\n",
    "    \n",
    "    # Step 1: Preprocess the seed text (same preprocessing done during training)\n",
    "    seed_text = seed_text.lower()  # Convert to lowercase (if that was part of your preprocessing)\n",
    "    \n",
    "    # Tokenize the seed text\n",
    "    seed_tokens = tokenizer.texts_to_sequences([seed_text])\n",
    "    \n",
    "    \n",
    "    seed_tokens_padded = tf.keras.preprocessing.sequence.pad_sequences(seed_tokens, maxlen=max_sequence_len, padding='pre')\n",
    "\n",
    "    generated_text = seed_text\n",
    "    \n",
    "    # Step 2: Generate text by predicting the next token iteratively\n",
    "    for _ in range(num_words_to_generate):\n",
    "        # Predict the next token probabilities\n",
    "        predictions = model.predict(seed_tokens_padded, verbose=0)\n",
    "\n",
    "     \n",
    "        predictions = predictions[0, :]  \n",
    "        predictions = predictions / temperature  \n",
    "        predictions = np.exp(predictions) / np.sum(np.exp(predictions))  # Softmax function for probabilities\n",
    "\n",
    "        \n",
    "        next_token = np.random.choice(len(predictions), p=predictions)\n",
    "\n",
    "        # Decoding the token to a word\n",
    "        next_word = tokenizer.index_word.get(next_token, '')\n",
    "        generated_text += ' ' + next_word\n",
    "        \n",
    "       \n",
    "        seed_tokens_padded = np.roll(seed_tokens_padded, shift=-1, axis=1)  \n",
    "        seed_tokens_padded[0, -1] = next_token  \n",
    "\n",
    "    return generated_text\n",
    "\n",
    "def print_in_lines(text, words_per_line=15):\n",
    "    words = text.split()\n",
    "    for i in range(0, len(words), words_per_line):\n",
    "        print(\" \".join(words[i:i + words_per_line]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "vukheta tiejente tihuvo minharhu lavakulu tk lowunsthwa veke cancel thlelo\n",
      "ngopfu tivonaka kwetlembetana tintshwa kamberile kahlekahle khorwisaka lokou tikumela xiyenganax\n",
      "rhekodiweke dyondziwa siveleka timbhoni kayetiwa khalikhuletiwa vukhongeri tlhandlekela wenae afrikadzonga\n",
      "vula\n"
     ]
    }
   ],
   "source": [
    "# seed text\n",
    "seed_text = \"vukheta\"  \n",
    "max_sequence_len = 30  # per tokenize sequence length\n",
    "\n",
    "\n",
    "generated_text = generate_text(model, tokenizer, seed_text, max_sequence_len, num_words_to_generate=30, temperature=1.0)\n",
    "\n",
    "# Printing the generated text in lines with 10 words per line\n",
    "print(\"Generated Text:\")\n",
    "print_in_lines(generated_text, words_per_line=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
